gbmTrainPredictions = predict(object = gbmModel,
newdata = test.s,
n.trees = 30,
type = "response")
cm=confusionMatrix(data = as.factor(gbmTrainPredictions),
reference = as.factor(test.s$target), positive = "x1")
}
if (method =="nb")
{
library(e1071)
train.s=d.m2(train.s)
test.s = d.m2(test.s)
model <- naiveBayes(target ~ ., data = train.s)
class(model)
summary(model)
preds <- predict(model, newdata = test.s)
cm=confusionMatrix(data = as.factor(preds), reference = as.factor(test.s$target), positive = "x1")
}
if (method=="knn")
{
library(class)
train.s=d.m2(train.s)
test.s = d.m2(test.s)
kn1 <- knn(train = train.s[-1], test = test.s$target,
cl = train.s$target, k=5,prob=TRUE)
cm=confusionMatrix(data = as.factor(kn1), reference = as.factor(test.s$target), positive = "x1")
}
if (method=="lr")
{
train.s=d.m2(train.s)
test.s = d.m2(test.s)
l1<-glm(target ~.,family=binomial(link='logit'),data=train.s)
lr1 <- predict(l1,newdata=test.s$target,type='response')
glm1 <- ifelse(lr1 > 0.5,"x1","x0")
cm=confusionMatrix(data = as.factor(glm1), reference = as.factor(test.s$target), positive = "x1")
}
if (method == "svm")
{
library(e1071)
train.s=d.m2(train.s)
test.s = d.m2(test.s)
svm_model <- svm(target ~ ., data=train.s)
pred <- predict(svm_model,test.s)
cm=confusionMatrix(data = as.factor(pred), reference = as.factor(test.s$target), positive = "x1")
}
if (method =="adaboost")
{
library(adabag);
adaboost<-boosting(target~., data=train.s, boos=TRUE, mfinal=20,coeflearn='Breiman')
pred = predict(adaboost,test.s)$class
cm=confusionMatrix(data = as.factor(pred), reference = as.factor(test.s$target), positive = "x1")
}
if (method =="fadaboost")
{
library(fastAdaboost);
test_adaboost <- adaboost(target~., train.s, 7)
pred <- predict(test_adaboost,newdata=test.s)$class
cm=confusionMatrix(data = as.factor(pred), reference = as.factor(test.s$target), positive = "x1")
}
return(cm)
}
baseline1.1<-function(df.train, df.test, model, f.measure = FALSE)
# meth = vector of method. Current: dt, rf, lr, nn, svm
# df.train and df.test must have binary target, naming "target", taking values x0 and x1
{
if (f.measure == FALSE)
{
le = length(model)
perf = matrix(0, nrow = 4, ncol = le)
rownames(perf)=c("Acc", "Sens", "Specs", "Balanced")
for (i in 1:le)
{
cmm=pd(model[i], df.train, df.test);
perf[,i]= c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11])
}
colnames(perf)=model
}
if(f.measure == TRUE)
{
le = length(model)
perf = matrix(0, nrow = 5, ncol = le)
rownames(perf)=c("Acc", "Sens", "Specs", "Balanced","F-Measure")
for (i in 1:le)
{
cmm=pd(model[i], df.train, df.test);
print(model[i])
perf[,i]= c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11], cmm$byClass[7])
}
colnames(perf)=model
}
return(perf)
}
baseline1.2<-function(df.train, df.test, model, f.measure = FALSE)
# meth = vector of method. Current: dt, rf, lr, nn, svm
# df.train and df.test must have binary target, naming "target", taking values x0 and x1
{
if (f.measure == FALSE)
{
le = length(model)
perf = matrix(0, nrow = 4, ncol = le)
rownames(perf)=c("Acc", "Sens", "Specs", "Balanced")
for (i in 1:le)
{
cmm=pd(model[i], df.train, df.test);
perf[,i]= c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11])
}
colnames(perf)=model
}
if(f.measure == TRUE)
{
le = length(model)
perf = matrix(0, nrow = 5, ncol = le)
rownames(perf)=c("Acc", "Sens", "Specs", "Balanced","F-Measure")
for (i in 1:le)
{
cmm=pd(model[i], df.train, df.test);
print(model[i])
perf[,i]= c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11], cmm$byClass[7])
}
colnames(perf)=model
}
perf = as.data.frame(perf)
perf$metric = rownames(perf)
rownames(perf) = NULL
library(tidyr)
perf = pivot_longer(perf, -metric, names_to = 'model')
return(perf)
}
chooseBestModel <- function(x)
{
tabulatedOutcomes <- table(x)
sortedOutcomes <- sort(tabulatedOutcomes, decreasing=TRUE)
mostCommonLabel <- names(sortedOutcomes)[1]
mostCommonLabel}
# Undersampling + Majority Voting
under.ensemble.ranger2 <-function(trainn, testt, repp = 10, mm = 1)
{
# ensemble repp undersample from the majority class to create repp balanced training set
# build repp models on repp training data
# ensemble repp models by voting
# testt could be balanced or unbalanced
library(caret)
library(ranger)
i0 = trainn[trainn$target=="x0",]
i1 = trainn[trainn$target=="x1",]
set.seed(2019)
# create a random matrix
u = replicate(repp, sample(1:nrow(i0), size = round(mm*nrow(i1))))
# initiate prediction matrix
pp = data.frame(matrix(0, nrow = nrow(testt), ncol = repp))
for (ii in 1:repp)
{
print(ii)
i00 = i0[u[,ii],]
# create a balanced training df1
df1 = rbind(i00, i1)
modell <- ranger(target~.,data =df1)
pred = predict(modell, testt)
pp[,ii]=pred$predictions
}
pred.vote = apply(t(pp), 2, chooseBestModel)
cmm=confusionMatrix(data = factor(pred.vote), reference = testt$target, positive = "x1")
perf= c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11], cmm$byClass[7])
return(perf)
}
over.ranger <-function(trainn=train, testt=test, mm = 1, method='ROS')
{
# oversampling with different ratio mm
library(caret)
library(ranger)
i0 = trainn[trainn$target=="x0",]
i1 = trainn[trainn$target=="x1",]
set.seed(2019)
i0.over = i0[sample(1:nrow(i0), round(mm*nrow(i1))),]
if(method=='ROS')
{
i1.over = i1[sample(1:nrow(i1), round(mm*nrow(i1)), replace=TRUE),]
train.over = rbind(i0.over, i1.over)
}
if(method=='SMOTE')
{
library(smotefamily)
train.over = rbind(i0.over, i1)
train.over = SMOTE(train.over[,-1], train.over[,1])$data
names(train.over)[length(names(train.over))]='target'
}
if(method=='BLSMOTE')
{
library(smotefamily)
train.over = rbind(i0.over, i1)
train.over = BLSMOTE(train.over[,-1], train.over[,1])$data
names(train.over)[length(names(train.over))]='target'
}
if(method=='ANS')
{
library(smotefamily)
train.over = rbind(i0.over, i1)
train.over = ANS(train.over[,-1], train.over[,1])$data
names(train.over)[length(names(train.over))]='target'
}
if(method=='DBSMOTE')
{
library(smotefamily)
train.over = rbind(i0.over, i1)
train.over = DBSMOTE(train.over[,-1], train.over[,1])$data
names(train.over)[length(names(train.over))]='target'
}
modell <- ranger(target~.,data =train.over)
pred = predict(modell, testt)
pp=pred$predictions
cmm=confusionMatrix(data = factor(pp), reference = testt$target, positive = "x1")
perf= c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11], cmm$byClass[7])
return(perf)
}
## Plot
plot.over = function(no, trainn=train, testt=test, method='ROS')
{
# Plot overfitting for different oversampling method
b.over.train = data.frame()
b.over.test = data.frame()
for (mm in no)
{
print(paste0(method,'  ',mm))
b.over.train = rbind(b.over.train, over.ranger(trainn=trainn, testt=trainn, mm=mm, method=method))
b.over.test = rbind(b.over.test, over.ranger(trainn=trainn, testt=testt, mm=mm, method=method))
}
names(b.over.train)=c('Accuracy', 'Sensitivity', 'Specificity', 'Balanced_Accuracy', 'F1' )
names(b.over.test)=c('Accuracy', 'Sensitivity', 'Specificity', 'Balanced_Accuracy', 'F1' )
b.over.train['Data']=c('Train')
b.over.test['Data']=c('Test')
b.over.train['mm']=no
b.over.test['mm']=no
b.over = rbind(b.over.train, b.over.test)
for(i in c(1:4)){
print(i)
nam = names(b.over)[i]
pl = ggplot(b.over, aes(x = mm, y = b.over[,i], color = Data))+geom_point()+ylab(nam)
ggsave(paste0(nam,'_',method,'.png'), pl)}
return(b.over)
}
result1 = function(no=1000, id = data)
# Grapth Balanced Accuracy vs. mm
# Return the Balanced Accuracy by mm
{
set.seed(2019)
library(caret)
splitIndex <- createDataPartition(id$target, p = .7, list = FALSE, times = 1)
train <- id[ splitIndex,]
test <- id[-splitIndex,]
ba_train0 = data.frame()
ba_test0 = data.frame()
for ( i in (1:no)){
print('==========================')
print(i)
ba_test0 = rbind(ba_test0, under.ensemble.ranger2(train, test, mm = i/100))
ba_train0 = rbind(ba_train0, under.ensemble.ranger2(train, train, mm = i/100))
}
# Graphing the results
bt0 = ba_train0
bte0 = ba_test0
# bt1 = ba_train
# bte1 = ba_test
#
# Add the data column
bt0$data = rep('Train', nrow(bt0))
bte0$data = rep('Test', nrow(bte0))
# bt1$data = rep('Train', nrow(bt1))
# bte1$data = rep('Test', nrow(bte1))
#
# Rename the culumns
names(bt0) = c('Accuracy', "Sensitivity", "Specificity", "Balanced_Accuracy",
"F1", "data")
names(bte0) = c('Accuracy', "Sensitivity", "Specificity", "Balanced_Accuracy",
"F1", "data")
# names(bt1) = c(names(c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11],
#                        cmm$byClass[7])), "data")
# names(bte1) = c(names(c(cmm$overall[1], cmm$byClass[1], cmm$byClass[2], cmm$byClass[11],
#                         cmm$byClass[7])), "data")
#
# Add the balanced ratio column (mm)
bt0$mm = c(1:no)/100
bte0$mm = c(1:no)/100
# bte1$mm = c(1:no)/100
# bt1$mm = c(1:no)/100
#
# Add the kk column (kk = 1 means predict bankruptcy 1 year ahead,
# kk = 0 means predicting bankrutcy in the same year)
# bt0$kk = rep(0, no)
# bte0$kk = rep(0, no)
#
# bte1$kk = rep(1, no)
# bt1$kk = rep(1, no)
#
# Combine all the result in 1 variable
b = rbind(bt0, bte0)
# Some Plots
# b %>% filter(data == 'Test') %>% ggplot(aes(x = mm, y = Balanced_Accuracy, color = factor(kk)))+
#   geom_point()
#
g1 = ggplot(b, aes(y=Balanced_Accuracy, x=mm, color = data)) +
geom_point()
ggsave(paste0('g1', rnorm(1), '.png'), g1)
return(b)
}
result2 = function(repp = 200, id = data)
# Plot barchart of all resampling methods
# Return the performance matrix
{
set.seed(2019)
library(caret)
splitIndex <- createDataPartition(id$target, p = .7, list = FALSE, times = 1)
train <- id[ splitIndex,]
test <- id[-splitIndex,]
#Balancing data
library(ROSE)
library(smotefamily)
#oversampling
i1 = train[train$target=='x1',]
i0 = train[train$target=='x0',]
i11 = i1[sample(1:nrow(i1), nrow(i0), replace = TRUE),]
train.o <- rbind(i11, i0)
#undersampling
#oversampling
i00 = i0[sample(1:nrow(i0), nrow(i1), replace = TRUE),]
train.u <- rbind(i1, i00)
#SMOTE
library(smotefamily)
train.s = SMOTE(train[,-1], train[,1])$data
names(train.s)[length(names(train.s))]='target'
#ANS
train.ans = ANS(train[,-1], train[,1])$data
names(train.ans)[length(names(train.ans))]='target'
#BLS
train.bls = BLSMOTE(train[,-1], train[,1])$data
names(train.bls)[length(names(train.bls))]='target'
#dbs
train.dbs = DBSMOTE(train[,-1], train[,1])$data
names(train.dbs)[length(names(train.dbs))]='target'
#Run Models
meth = 'ranger'
base.results <- baseline1.1(train, test, meth)
undersample.results <- baseline1.1(train.u, test, meth)
oversample.results <- baseline1.1(train.o, test, meth)
smote.results <- baseline1.1(train.s, test, meth)
bls.results <- baseline1.1(train.bls, test, meth)
dbs.results <- baseline1.1(train.dbs, test, meth)
ans.results <- baseline1.1(train.ans, test, meth)
bag_un = under.ensemble.ranger2(train, test, mm=1.25, repp = repp)
##
#Plot the result
b1 = rbind(t(base.results),
t(oversample.results),
t(smote.results),
t(bls.results),
t(dbs.results),
t(ans.results),
t(undersample.results),
t(bag_un[1:4])
)
row.names(b1)=NULL
b1 = as.data.frame(b1)
b1$method = c('Original', 'ROS', 'SMOTE', 'BLSMOTE', 'DBSMOTE','ANS','Under','BU')
return(b1)
}
no = c(1.5,c(2:183))
for (med in c('ROS', 'ANS','SMOTE','DBSMOTE')){
plot.over(no=no, method=med)
}
version
install.packages('xaringan')
devtools::install_github("gadenbuie/xaringanExtra")
install.packages('devtools')
devtools::install_github("gadenbuie/xaringanExtra")
3+4
3^2
3^2+log(10)+3/2
6^2+10+2
x <- 3
x
y <- c(1, 2, 5)
z <- c('Peter','Julie','Ken')
class(x)
class(y)
class(z)
?class
a <- 2020
b = 2020
x <- c(1, 2, 5)
y <- c(10, 0, 7)
x+y
x-y
x*y
x/y
x
x^2
log(x)
?log
mean(x)
sum(x)
median(x)
sd(x)
min(x)
max(x)
summary(x)
?summary
x <- c(1:2020)
x
x <- c(-2020:2020)
x
x <- rep(1, 1000)
x
x <- rep(c(1,2), 1000)
x
?c
x <- c(1,1,2,3)
x <- (1,1,2,3)
x <- c(1:2020)
sum(x)
sum(c(1:2020))
x <- c(1:2020)
sum(x^2)
sum(x^3)/2020
mean(x^3)
?mean
c(1:10)
seq(1:11, 2)
seq(1, 11, 2)
seq(1, 11, 2)
seq(1, 100, 2)
?seq
x <- c(1, 2019, 2)
x
x <- seq(1, 2019, 2)
x
y <- seq(2, 2020, 2)
x*y
sum(x*y)
x <- c(1:2020)
y <- c(2:2021)
sum(x/y)
x <- c(1:100)^2
x
1/x
sum(1/x)
x <- c(1:1000)^2
sum(1/x)
x <- c(1:10000)^2
sum(1/x)
x <- c(1:100000)^2
sum(1/x)
x <- c(1:2020)
x <- c(1:2020)
# How to create a sequence
x <- rep(c(1,2), 1000)
seq(1:11, 2)
seq(1, 11, 2)
seq(1, 11, 2)
seq(1, 100, 2)
x <- rep(c(1,2), 1000)
seq(1:11, 2)
seq(1, 11, 2)
seq(1, 11, 2)
seq(1, 100, 2)
x <- c(1:2020)
sum(x^2020)
library(tidyverse)
df <- read_csv('https://covidtracking.com/data/download/all-states-history.csv')
class(df)
df1 <- read.csv('https://covidtracking.com/data/download/all-states-history.csv')
class(df1)
str(df)
df$death
df[1,1]
df1
df
df <- read.csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')
tb <- read_csv('https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv')
str(df)
str(tb)
?read_csv
tb
filter(tb, state=='Rhode Island')
tb[tb$state=='Rhode Island',]
filter(tb, state=='Rhode Island', county=='Providence')
tb[(tb$state=='Rhode Island') & (tb$county=='Providence'),]
tb
df <- read_csv('https://covidtracking.com/data/download/all-states-history.csv')
df
# Calculate the average number of cases by dataQualityGrade
by(df$dataQualityGrade, df$positiveIncrease, mean)
by(df$positiveIncrease, df$dataQualityGrade, mean)
d1 = group_by(df, dataQualityGrade)
summarise(d1, mean(positiveIncrease))
summarise(group_by(df, dataQualityGrade), mean(positiveIncrease))
df %>%
group_by(dataQualityGrade) %>%
summarise(mean(positiveIncrease))
x <- c(1:10)
mean(x)
x %>% mean
log(mean(x))
x %>% mean %>% log
log(16, 2)
16 %>% log(2)
filter(df, state=='RI')
df %>% filter(state=='RI')
# Calculate the average number of cases by dataQualityGrade in Rhode Island
d1 <- df[df$state=='RI',]
by(d1$positiveIncrease, d1$dataQualityGrade, mean)
df %>% filter(state=='RI') %>% group_by(dataQualityGrade) %>% summarise(mean(positiveIncrease))
getwd()
setwd("C:/Users/snguyen4/Dropbox/git/math460/beamer")
