---
title: "Overfitting and Model Tuning"
output:
  beamer_presentation: default
  powerpoint_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

## Overfitting and Model Tuning

author: Son Nguyen
<!-- font-family: Garamond -->


## Reading Materials

- Max Kuhn. Chapter 4. 


## Prediction Problem

Given data of $X = [X_1, X_2, ..., X_d]$ and $Y$. Find the relation between $X$ and $Y$. 

## Prediction Problem - Examples


- One Input Variable $X$

<center>

| X <br>  | Y <br> |
|----|----|
| 13  | 4.0  |
| 6  | 3.5 |
| 14 | 3  |
| 10 | 3.9 |
| 7  | 2.7 |
| 12 | 3.8 |
| 1 | 1.5 |

</center>

How are $X$ and $Y$ related?

## Prediction Problem - Examples

- Multiple Input Variables

<center>

| $X_1$ | $X_2$ | ... | $X_{35}$ | $Y$ |
|-----|-----|-----|-----|---|
| 1     | -1    | ...    |  2   | Tree  |
|  2.1   |    0 |   ...  |    6 | Not a Tree  |
|   3  |     0| ...    |    8 | Tree  |

</center>

How are $X$ and $Y$ related?

## Prediction Problem

- If $Y$ is continous, we have a **regression** problem.
- If $Y$ is categorical, we have a **classification** problem.
- If $Y$ is binary, we have a **binary classification** problem.

## Prediction Problem - Examples


- This is a regression problem since $Y$ is continuous. 

<center>

| X <br>  | Y <br> |
|----|----|
| 13  | 4.0  |
| 6  | 3.5 |
| 14 | 3  |
| 10 | 3.9 |
| 7  | 2.7 |
| 12 | 3.8 |
| 1 | 1.5 |

</center>

## Prediction Problem - Examples

- This is a binary classification Problem since $Y$ is binary. 

<center>

| $X_1$ | $X_2$ | ... | $X_{35}$ | $Y$ |
|-----|-----|-----|-----|---|
| 1     | -1    | ...    |  2   | Tree  |
|  2.1   |    0 |   ...  |    6 | Not a Tree  |
|   3  |     0| ...    |    8 | Tree  |

</center>

## Overfitting

- Consider the data:

<center>

| X <br>  | Y <br> |
|----|----|
| 13  | 4.0  |
| 6  | 3.5 |
| 14 | 3  |
| 10 | 3.9 |
| 7  | 2.7 |
| 12 | 3.8 |
| 1 | 1.5 |

</center>
- We will fit these data by polynomial model. 
- In polynomial model, $Y$ is a polynomial function of $X$. 

## Overfitting


<center>
```{r, echo=FALSE}
library(tidyverse)
x<-c(13, 6, 14, 10, 7, 12, 1)
y<-c(4, 3.5, 3, 3.9, 2.7, 3.8, 1.5)

d1 = data.frame(x=c(5,7,9,6,12), y=c(2.6,3.5,4.0, 3.7, 5.0))
d2=data.frame(x=c(20, 10, 1, 13, 17, 2), y=c(4, 3.5, 1, 3, 4, 3.8))

d <- as_tibble(cbind(x, y))
names(d1)=c("x","y")

ggplot(d, aes(x=x,y=y))+geom_point()
```
</center>

- We will fit these data by **polynomial model**. 
- In polynomial model, $Y$ is a polynomial function of $X$. 


## Overfitting -  Polynomial Model

- In polynomial model, we need to specify the degree of the polynomial, $n$.  Let try a few. 
- If $n=1$, we have a familiar **linear model**. 
- **Question: Does increasing $n$ resuls in a better model?**

## Overfitting -  Polynomial Model

- $n=1$. 

<center>
```{r, echo=FALSE}
a1 =lm(y~x)
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 1), se = FALSE)+labs(title=paste("Error = ", toString(1-summary(a1)$r.squared)))
```
</center>

## Overfitting -  Polynomial Model

- $n=2$. 

<center>
```{r, echo=FALSE}
a2 =lm(y~x+I(x^2))
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE)+labs(title=paste("Error = ", toString(1-summary(a2)$r.squared)))
```
</center>

## Overfitting -  Polynomial Model

- $n=3$. 

<center>
```{r, echo=FALSE}
a3 =lm(y~x+I(x^2)+I(x^3))
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 3), se = FALSE)+labs(title=paste("Error = ", toString(1-summary(a3)$r.squared)))
```
</center>

## Overfitting -  Polynomial Model

- $n=4$. 

<center>
```{r, echo=FALSE}
a4 =lm(y~x+I(x^2)+I(x^3)+I(x^4))
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 4), se = FALSE)+labs(title=paste("Error = ", toString(1-summary(a4)$r.squared)))
```
</center>

## Overfitting -  Polynomial Model

- $n=5$. 

<center>

```{r, echo=FALSE}
a5 =lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5))
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 5), se = FALSE)+labs(title=paste("Error = ", toString(1-summary(a5)$r.squared)))
```
</center>

## Overfitting -  Polynomial Model

- $n=6$. 

<center>

```{r, echo=FALSE}
a6 = lm(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6))
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 6), se = FALSE)+labs(title=paste("Error = ", toString(1-summary(a6)$r.squared)))
```
</center>

## Overfitting -  Polynomial Model

- **Question**: What are the errors when $n>6$?

## Overfitting -  Polynomial Model

- **Question**: What are the errors when $n>6$?
- **Answer**: The errors are all zeros. (There are actually many solutions for each degree greater than 6.) 

## Overfitting -  Polynomial Model

- **Question**: What is the best model? 

## Overfitting -  Polynomial Model

- **Question**: What is the best model? 
- **Answer**: We do not know.  We need a validation dataset to validate the models. 


## Overfitting -  Polynomial Model

- The errors we have seen are called **training errors**


## Overfitting -  Polynomial Model

- Let's validate these models with a validation dataset
- Validation Data

<center>

| X <br>  | Y <br> |
|----|----|
| 5  | 2.6  |
| 7  | 3.5 |
| 9 | 4.0  |
| 6 | 3.7 |
| 12  | 5.0 |

</center>

## Overfitting -  Polynomial Model

- $n=1$. 

<center>
```{r, echo=FALSE}
e1 = sum((predict(a1, d1)-d1$y)^2)
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 1), se = FALSE)+labs(title=paste("Training Error = ", toString(1-summary(a1)$r.squared),"Validation error = ",toString(e1)))+geom_point(d1, mapping = aes(x=x, y=y, color = "red"))
```
</center>

## Overfitting -  Polynomial Model

- $n=2$. 

<center>
```{r, echo=FALSE}
e2 = sum((predict(a2, d1)-d1$y)^2)
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE)+labs(title=paste("Training Error = ", toString(1-summary(a2)$r.squared),"Validation error = ",toString(e2)))+geom_point(d1, mapping = aes(x=x, y=y, color = "red"))
```
</center>

## Overfitting -  Polynomial Model

- $n=3$. 

<center>
```{r, echo=FALSE}
e3 = sum((predict(a3, d1)-d1$y)^2)
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 3), se = FALSE)+labs(title=paste("Training Error = ", toString(1-summary(a3)$r.squared),"Validation error = ",toString(e3)))+geom_point(d1, mapping = aes(x=x, y=y, color = "red"))
```
</center>

## Overfitting -  Polynomial Model

- $n=4$. 

<center>
```{r, echo=FALSE}
e4 = sum((predict(a4, d1)-d1$y)^2)
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 4), se = FALSE)+labs(title=paste("Training Error = ", toString(1-summary(a4)$r.squared),"Validation error = ",toString(e4)))+geom_point(d1, mapping = aes(x=x, y=y, color = "red"))
```
</center>
## Overfitting -  Polynomial Model

- $n=5$. 

<center>

```{r, echo=FALSE}
e5 = sum((predict(a5, d1)-d1$y)^2)
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 5), se = FALSE)+labs(title=paste("Training Error = ", toString(1-summary(a5)$r.squared),"Validation error = ",toString(e5)))+geom_point(d1, mapping = aes(x=x, y=y, color = "red"))
```
</center>

## Overfitting -  Polynomial Model

- $n=6$. 

<center>

```{r, echo=FALSE}
e6 = sum((predict(a6, d1)-d1$y)^2)
ggplot(d, aes(x=x, y=y))+geom_point()+geom_smooth(method = "lm", formula = y ~ poly(x, 6), se = FALSE)+labs(title=paste("Training Error = ", toString(1-summary(a6)$r.squared),"Validation error = ",toString(e6)))+geom_point(d1, mapping = aes(x=x, y=y, color = "red"))
```
</center>

## Overfitting -  Polynomial Model

- Training Error vs. Validation Error

<center>

|   | Training Error | Validation Error |
|---|----------------|---------------|
| $n=1$  | 0.4443277      | 3.726484      |
| $n=2$  | 0.2104958      | 2.304728      |
| $n=3$  | 0.1724256      | **1.955191 (Best!)**      |
| $n=4$  | 0.08719074     | 2.515661      |
| $n=5$  | 0.05131475     | 5.987636      |
| $n=6$  | 0              | 18.24475      |

</center>

## Overfitting -  Polynomial Model

- Training Error vs. Validation Error

<center>
```{r, echo=FALSE}
train = c(1-summary(a1)$r.squared,
                1-summary(a2)$r.squared,
                1-summary(a3)$r.squared,
                1-summary(a4)$r.squared,
                1-summary(a5)$r.squared,
                1-summary(a6)$r.squared)
validation =c(e1,e2,e3,e4,e5,e6)
result = data.frame(n=c(1:6), train, validation)
library(reshape2)
result = melt(result, id = "n")
names(result)=c("n","data", "error")
ggplot(result)+geom_point(mapping = aes(x=n, y= error, color = data))
```
</center>

## Overfitting -  Polynomial Model

- As the degree $n$ increases, the training errors decrease
- Model 6 ($n=6$) is the best (perfect) in training but the worst in validation.  
- The best model is the model with the best (lowest) error in **validation data**.

Overfitting -  Polynomial Model
##
- Model 4, 5 and 6 are **overfitted**
- Model 1 and 2 are **underfitted**
- Model 3 is the best model. 

## Overfitting in Regression

![](overfit3.png)


## Overfitting in Classification

![](overfit2.png)

## Model Complexity/Capacity

- In polynomial models, the larger $n$, the more complex/capable the model. 

- Model complexity can be measured by the number of parameters/unknown of the model. 


## Model Complexity/Capacity

- Linear model: 

$$
y = ax+b
$$

- **Question**: How many unknowns/parameters in linear model?

## Model Complexity/Capacity

- Linear model $(n=1)$: 

$$
y = ax+b
$$

- **Question**: How many unknowns/parameters in linear model?
- **Answer**: Two unknowns/paramters: $a$ and $b$

## Model Complexity/Capacity

- Quadratic model $(n=2)$: 

$$
y = ax^2+bx+c
$$

- Three unknowns: $a$, $b$, and $c$. 
- Quadratic model has **more unknowns/parameters** then linear model. Thus, quadratic model is **more complex** than linear model

## Model Complexity/Capacity


- The mode complex the model, the easier it becomes overfitted! 

## Model Complexity/Capacity

![](complexity.png)

## Model Tuning

- We just "tuned" the the parameter $n$. 
- The parameter $n$ is called **tuning parameter**, or **hyperparameter**


## Model Tuning

- Model tuning is the process of finding the **best** values for the tuning parameters of the model
- This is done through **trying out** many values for the tuning parameters then select the best values. 

## Model Training

- Model training is the process of finding the unknown/parameters of the model
- **Example**:  Training linear model $y= ax+b$ is to find $a$ and $b$ that best fit the data

## Model Training vs. Model Tuning

- Model training finds the **parameters**
- Model tuning finds **hyperparameters**



## Model vs. Family of Models

- The polynomial model is a **family** of models.   
- Linear model is just **one** model
- A family of models has "tuning parameters". 
- A single model, say, linear model, does not have tuning parameter
- Some model has multiple tuning parameters. 

## Data Splitting

- We need validation data for model tuning.
- **Question**: How can one of obtain validation data? 

## Data Splitting

- We need validation data for model tuning.
- **Question**: How can one of obtain validation data? 
- **Answer**:  We do not use the entire data to train models. We use a portion of it for training and save data for validation and testing. 


## Data Splitting:  Train-Validation-Test

![](train.png)


## k-folds Cross Validation

![](kfold.png)

## k-folds Cross Validation

![](kfold3.png)


## k-folds Cross Validation and test

![](valid.png)
