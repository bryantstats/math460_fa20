---
title: "Untitled"
output:
  word_document: default
  beamer_presentation: default
  pdf_document: default
---

Measuring Performance in Classification Models
========================================================
author: Son Nguyen
font-family: Garamond


Reading Materials
=======================================================
- Max Kuhn. Chapter 11. 


Two outcomes of classification models
=======================================================
- Predicted Probabilities
- Class Prediction

Examples
=======================================================

- Predicting if a passenger in the titanic is survived or not survived
- The outcome could look like this. 

<center>

```{r, echo = FALSE}
library(e1071)
ss = c(6,  2, 12,  7, 11,  3, 10,  4,  1,  8,  9,  5)
p <- c(.01, .2, .35,.38, .45, .55, .63, .68, .71,.84, .9, .94)
t <- c(0 ,1, 0, 0,1,1, 0,1,1,0,1 ,1)
t1 = ifelse(t==1, "Survived", "Not Survived")
pred = ifelse(p>.5, "Survived", "Not Survived")

d2 = as.data.frame(cbind(p,pred, t1))
d3 = cbind(c(1:12),d2[ss,])
names(d3) = c("Passenger ID","Probability of Survived","Prediction")
knitr::kable(d3[,c(1:3)], row.names=FALSE)
```

Examples
=======================================================

- Notice that this model predicts "Survived" for passengers with the probabilities of being greater than 0.5
- 0.5 is called **cut-off value**. 
- The cuff-off value is set by 0.5 by default. 
- The cut-off value can be changed by the modeler. 

Confusion Matrices
=======================================================

<center>


|                 | Predicted Positive | Predicted Negative |
|-----------------|--------------------|--------------------|
| __Actual Positive__ |  True Positive (TP)         |  False Negative (FN)         |
| __Actual Negative__ |    False Positive  (FP)     |    True Negative (TN)        | 
</center>


Confusion Matrices
=======================================================

<center>
![](cm.png)
</center>

Confusion Matrices - Example
=======================================================
- "Survived" = **"Positive"**
- "Not Survived" = **"Negative"**

<center>

```{r, echo = FALSE}


names(d3)[4] = c("Truth")
d3$Evaluation = c("TP","FN","TP","FP","TP","TN","FP","TN","TN","TP","TP","FN")
knitr::kable(d3, row.names = FALSE)

```
</center>



Confusion Matrices
=======================================================

<center>


|                 | Predicted Positive | Predicted Negative |
|-----------------|--------------------|--------------------|
| __Actual Positive__ |  5         |  2        |
| __Actual Negative__ |    2     |    3        | 
</center>

Model evaluation from Confusion Matrices
=======================================================

<center>


|                 | Predicted Positive | Predicted Negative |
|-----------------|--------------------|--------------------|
| __Actual Positive__ |  True Positive (TP)         |  False Negative (FN)         |
| __Actual Negative__ |    False Positive  (FP)     |    True Negative (TN)        | 
</center>

$$
\text{Misclassification Rate} &=  \frac{FN + FP}{\text{Total}} = \frac{FN + FP}{TN+TP+FN+FP} \\
\text{Accuracy} &= \frac{TN+TP}{TN+TP+FN+FP} \\
\text{Sensitivity} &= \frac{TP}{\text{Actual Positive}} = \frac{TP}{TP+FN} \\ 
\text{Specificity} & = \frac{TN}{\text{Actual Negative}} = \frac{TN}{TN+FP} \\
\text{Precision} &= \frac{TP}{TP+FP} \\
\text{F1-Score} &=2\cdot  \frac{\text{Precision} \cdot \text{Sensitivity}}{\text{Precision + Sensitivity}} = \frac{2TP}{2TP+FN+FP}

$$

Confusion Matrices
=======================================================

<center>


|                 | Predicted Positive | Predicted Negative |
|-----------------|--------------------|--------------------|
| __Actual Positive__ |  TP = 5         |  FN = 2        |
| __Actual Negative__ |   FP = 2     |    TN = 3        | 
</center>
$$
\text{Misclassification Rate} &=4/12 \\
\text{Accuracy} &= 8/12 \\
\text{Sensitivity} &= 5/7 \\ 
\text{Specificity} & = 3/5 \\
\text{Precision} &= 5/7 \\
\text{F1-Score} &= 5/7

$$

ROC Curves
=======================================================
- Notice that all of the measures calculated in the last slide are based on the **cut-off 0.5**

- What if we change the cut-off value, **c**?

ROC Curves
=======================================================
- What is the best cut-off value?

<center>
```{r, echo = FALSE}
library(caret)
p <- c(.01, .2, .35,.38, .45, .55, .63, .68, .71,.84, .9, .94)
t <- c(0 ,1, 0, 0,1,1, 0,1,1,0,1 ,1)
c.o <- c(0, .1, .2, .3, .4, .5, .6, .7, .8 , .9, 1)
f = function(x){
  pp <- p>x
  cm = confusionMatrix(as.factor(as.numeric(pp)), 
                       as.factor(t), positive = "1")
  rt = c(cm$byClass[1], 1- cm$byClass[2])
  names(rt)=c("Sensitivity", "Spec")
  return(rt)
}

d = as.data.frame(t(sapply(c.o, f)))
label = sapply(c.o, function(x){paste("c =", x)})

d1 = cbind(label, d)
d1[,3]=1-d1[,3]
names(d1)[c(1,3)]=c("Cut-off Values","Specificity")

knitr::kable(d1)
```

</center>

ROC
=======================================================
- **Question**: What is the best cut-off value?

<center>
```{r, echo = FALSE}
library(caret)
p <- c(.01, .2, .35,.38, .45, .55, .63, .68, .71,.84, .9, .94)
t <- c(0 ,1, 0, 0,1,1, 0,1,1,0,1 ,1)
c.o <- c(0, .1, .2, .3, .4, .5, .6, .7, .8 , .9, 1)
f = function(x){
  pp <- p>x
  cm = confusionMatrix(as.factor(as.numeric(pp)), 
                       as.factor(t), positive = "1")
  rt = c(cm$byClass[1], 1- cm$byClass[2])
  names(rt)=c("Sensitivity", "Spec")
  return(rt)
}

d = as.data.frame(t(sapply(c.o, f)))
label = sapply(c.o, function(x){paste("c =", x)})

ggplot(d,aes(x = Spec, y = Sensitivity))+geom_point()+geom_text(aes(label= label ,hjust=.5, vjust=-.5))+ xlab("1- Specificity")
```

</center>

ROC Curve
=======================================================
- **Question**: What is the best cut-off value?
- **Answer**: $c = 0.4$ is the best cut-off value


ROC Curve
=======================================================
- Each cut-off value **c** results a pair of (1-Specificity, Sensitivity) or (TP Rate, FP Rate)
- The collections of all these pairs/points for all the cut-off values is the Receiver operating characteristic Curve (ROC Curve) 

ROC Curve of the example model
=======================================================

<center>
```{r, echo = FALSE}
co <- c(0:20)/20
f = function(x){
  pp <- p>x
  cm = confusionMatrix(as.factor(as.numeric(pp)), 
                       as.factor(t), positive = "1")
  rt = c(cm$byClass[1], 1- cm$byClass[2])
  names(rt)=c("Sensitivity", "Spec")
  return(rt)
}

d = as.data.frame(t(sapply(co, f)))
label = sapply(c.o, function(x){paste("c =", x)})

#d1 = d %>% arrange(Sensitivity)
library(tidyverse)
ggplot(d%>% arrange(Sensitivity),aes(x = Spec, y = Sensitivity))+geom_line()+ xlab("1- Specificity")
```

- The curve is not very smooth because the data is very small
- With bigger data, the ROC curve will be very "smooth"

ROC Curve 
=======================================================
<center>
![](images/roc2.png)
- The closer the curve to the point (0,1) the better the model
- The best cut-off value is at the point closest to (0,1)
- (0,1) is the **perfect point**, resulting 0 misclassification model.  
- At (0,0) the model predicts everything positive
- At (1,1) the model predicts everything negative
- The ROC of the random guess model is the diagonal 


ROC Curve 
=======================================================
<center>
![](roc3.jpg)

- AUC = Area Under the (ROC) Curve

ROC Curve 
=======================================================
<center>
![](roc4.png)


ROC Index
=======================================================
- ROC Index is the area under the ROC Curve 


ROC Index - Area Under the Curve (AUC)
=======================================================
- The closer the AUC to 1 the better the model
- The closer the AUC to 1/2 the worse the model
- Model with AUC = 1/2 is as good as a random guess or guessing by tossing a coin

- **Question**: What if the AUC less than 1/2? Are models with AUC less than 1/2 **useless**?

Another Question
=======================================================
- **Question**: Is the model with the misclassification rate of 100% the most **useless** model?

Answer
=======================================================
- **Question**: Is the model with the misclassification rate of 100% an useless model?
- *Answer*: No, by flipping the predictions of the models, one gets the **perfect model** with 0 misclassification rate. 


Back to the Question
=======================================================
- **Question**: What if the AUC less than 1/2? Are models with AUC less than 1/2 **useless**?

- **Answer**: Model with AUC less than 1/2 could be made to be better by flipping the predictions (if the model predicts positve, flip it to predict negative) 

Cumulative Lift 
=======================================================
- In the dataset, the ratio of "Survived" is 7/12 = 58.33%
- This mean that if we pick **randomly** a passenger in the this group, the chance of picking a "Survived" passenger is 58.33%
- **Question**: If we want to pick a "Survived" passenger, is there a better way than pick randomly? 

Cumulative Lift 
=======================================================
- **Question**: If we want to pick a "Survived" passenger, is there a better way than pick randomly? 
- **Answer**:  Yes, we should pick the one with the highest predictied probability.  

Cumulative Lift 
=======================================================

<center>
```{r, echo = FALSE}
library(tidyverse)
d5 = as_tibble(cbind(p,t))
d5 <- d5 %>% arrange(-p)
d5 <-cbind(c(1:12), d5)
names(d5)<-c("Order", "Predicted Probabilities", "True Values")

knitr::kable(d5)
```

- Pick randomly, "success rate" is 58.33%
- Pick the top 1, success rate is 1/1 = 100%
- We say, at 1/12 = 8.33%, the model lift is 100/58.33 = 1.71

Cumulative Lift 
=======================================================

<center>
```{r, echo = FALSE}
library(tidyverse)
d5 = as_tibble(cbind(p,t))
d5 <- d5 %>% arrange(-p)
d5 <-cbind(c(1:12), d5)
names(d5)<-c("Order", "Predicted Probabilities", "True Values")

knitr::kable(d5)
```

- Pick randomly, "success rate" is 58.33%
- Pick the top 2, success rate is 2/2 = 100%
- We say, at 2/12 = 16.67%, the model lift is 100/58.33 = 1.71

Cumulative Lift 
=======================================================

<center>
```{r, echo = FALSE}
library(tidyverse)
d5 = as_tibble(cbind(p,t))
d5 <- d5 %>% arrange(-p)
d5 <-cbind(c(1:12), d5)
names(d5)<-c("Order", "Predicted Probabilities", "True Values")

knitr::kable(d5)
```

- Pick randomly, "success rate" is 58.33%
- Pick the top 2, success rate is 2/2 = 100%
- We say, at 2/12 = 16.67%, the model lift is 100/58.33 = 1.71

Cumulative Lift 
=======================================================

<center>
```{r, echo = FALSE}
library(tidyverse)
d5 = as_tibble(cbind(p,t))
d5 <- d5 %>% arrange(-p)
d5 <-cbind(c(1:12), d5)
names(d5)<-c("Order", "Predicted Probabilities", "True Values")

knitr::kable(d5)
```

- Pick randomly, "success rate" is 58.33%
- Pick the top 3, success rate is 2/3 = 66.66%
- We say, at 3/12 = 25%, the model lift is 66.66/58.33 = 1.14


Cumulative Lift 
=======================================================

<center>
```{r, echo = FALSE}
library(tidyverse)
d5 = as_tibble(cbind(p,t))
d5 <- d5 %>% arrange(-p)
d5 <-cbind(c(1:12), d5)
names(d5)<-c("Order", "Predicted Probabilities", "True Values")

knitr::kable(d5)
```
- Pick randomly, "success rate" is 58.33%
- Pick the top 4, success rate is 3/4 = 75%
- We say, at 4/12 = 25%, the model lift is 75/58.33 = 1.28


Cumulative Lift 
=======================================================
<center>

```{r, echo = FALSE}
lift = data.frame(Percentage= c(1:12)/12, Lift= c(1,1,2/3,3/4,4/5, 4/6, 5/7, 6/8,6/9,6/10,7/11, 7/12)/(7/12))
knitr::kable(lift)
```

Cumulative Lift 
=======================================================
<center>

```{r, echo = FALSE}
ggplot(lift, aes(x = Percentage, y = Lift))+geom_line()+geom_hline(yintercept=1, linetype="dashed", color = "red", size=2)
```

Cumulative % Response 
=======================================================
<center>

```{r, echo = FALSE}
res = data.frame(Percentage= c(1:12)/12, Percent_Response = c(1,1,2/3,3/4,4/5, 4/6, 5/7, 6/8,6/9,6/10,7/11, 7/12))

knitr::kable(res)
```

Cumulative % Response 
=======================================================
<center>

```{r, echo = FALSE}
ggplot(res, aes(x = Percentage, y = Percent_Response))+geom_line()+ geom_hline(yintercept=7/12, linetype="dashed", color = "red", size=2)
```
